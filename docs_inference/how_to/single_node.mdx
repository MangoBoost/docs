---
sidebar_position: 3
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


# Deploying an Inference Service

One of the most powerful uses of the LLMBoost container is to deploy a containerized inference service compatible with the Kubernetes framework.
In this tutorial, we will start inference services manually using the LLMBoost container's command-line interface.
In practice, the commands are be packaged into a shell script so that the containerized services are started automatically when the container is launched.

### Step 0: Before you start

Start the LLMBoost container.

```bash
# Set environment variables
export MODEL_PATH=<absolute_path_to_model>
export LLMBOOST_IMAGE=<llmboost-docker-image-name>:<tag>
export HUGGING_FACE_HUB_TOKEN=<your_huggingface_token>
```
> ðŸ’¡ These variables are used when launching the Docker container to ensure correct model loading and authentication.

<Tabs>
  <TabItem value="nvidia" label="On NVIDIA" default>

```bash
docker run -it --rm \
  --network host \
  --gpus all \
  --group-add video \
  --ipc host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  -v $MODEL_PATH:/workspace/models \
  -w /workspace \
  -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \
  $LLMBOOST_IMAGE \
  bash
```
</TabItem>

<TabItem value="amd" label="On AMD">
```bash
docker run -it --rm \
  --network host \
  --group-add video \
  --ipc host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --device=/dev/dri:/dev/dri \
  --device=/dev/kfd:/dev/kfd \
  -v $MODEL_PATH:/workspace/models \
  -w /workspace \
  -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \
  $LLMBOOST_IMAGE \
  bash
```
</TabItem> 
</Tabs>

An LLMBoost container should start in an interactive command shell session. The commands in the rest of this tutorial should be entered into the command shell prompt within the container.

### Step 1: Starting an inference service

Start a LLMBoost container on the host node in which you want to run the inference server.
Simply type the following command to start a large language model inference service to respond to inference requests over the network.

```
llmboost serve --model_name meta-llama/Llama-3.1-8B-Instruct
```

After starting, the service will by default listen for inference requests on port 8011.
You can choose a different port by specifying it in an argument (e.g. --port 8012).
As before, you can name other large language models to use in the inference service.

It will take a few minutes for the service to be ready.
Wait until the service status message reports that it is ready before proceeding with the rest of the steps.

### Step 2: Using the inference service from a client

When the inference service is ready, you can connect to it by connecting to port 8011 of the host node.
We will later cover how to access LLMBoost inference servers using many of the standard clients and APIs over the network.
For now, we will use a simple client built into **llmboost** to connect to the inference service.

Start another LLMBoost container in interactive mode on the same host node.

```
# NOTE: Instead of starting a second container, you can attach to the currently running LLMBoost container after figuring out its DOCKER_ID from 'docker ps'
docker ps
docker exec -it <DOCKER_ID> bash
```

From the second LLMBoost container's shell prompt, type:

```
llmboost client ---port 8011
```

You can type questions into the client like before (e.g., "What is an LLM?").
Watch the status messages reported by the inference service in the first container.
You can see the actual requests and responses exchanged between the service and client.
Type ctrl-D to exit the client.

Instead of the command line client, it is also easy to access the inference service from an LLMBoost Python client API.

```python
from llmboost.entrypoints.client import send_prompt


response = send_prompt(
    host="localhost",
    port=8011,
    model_path="meta-llama/Llama-3.1-8B-Instruct",
    role="user",
    user_input="What is the most famous landmark of Seattle?"
)
```

To end this tutorial, type ctrl-C in the first container window to terminate the service.
You can also use **llmboost shutdown --port XXXX** to terminate the service associated with the specified port.
Or, you can use **llmboost shutdown --all** to shutdown all LLMBoost inference service instances on the host node.

:::tip Standard API Integration
Advanced users can visit [Using the OpenAI API](../deep_dive/openai_api) to see how to integrate LLMBoost powered inference with any OpenAI-compatible client or tool.
:::

### Step 3: Multi model deployment

On a multi-GPU server, LLMBoost can concurrently deploy multiple inference services based on different models.
To use this feature, you'll need a configuration file that specifies the deployment details for each model.
An example configuration file (config.yaml) is shown below. 

```
common:
  kv_cache_dtype: auto
  host: 127.0.0.1
  tp: 1

models:
  - model_path: meta-llama/Llama-3.1-8B-Instruct
    port: 8011
    dp: 1
  - model_path: microsoft/Phi-3-mini-4k-instruct
    port: 8012
    dp: 1
```

You can try out the above config.yaml in a server with at least 2 GPUs. (Please see the deep-dive page on [Tensor and Data Parallelism](../deep_dive/parallelism) for an explanation of the `tp` and `dp` parameters.) 
Initiate a multi-model deployment by typing:

```
llmboost deploy --config config.yaml
```

You can check the status of the deployments by running **llmboost status**, and will get similar output as below:

```
+------+------------------------+---------+
| Port |          Name          | Status  |
+------+------------------------+---------+
| 8011 | Llama-3.1-8B-Instruct  | running |
| 8012 | Phi-3-mini-4k-instruct | running |
+------+------------------------+---------+
```

You can use the same LLMBoost clients discussed above to connect to port 8011 or 8012, depending on which model you wish to service your request.

Please continue to the next tutorial to see how to start and manage inference services across a multi-node cluster.
