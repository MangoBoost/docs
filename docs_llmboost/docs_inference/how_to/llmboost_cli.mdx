---
index: llmboost_cli
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Using LLMBoost Interactively

In this introductory tutorial we will launch the LLMBoost container into an interactive command-line session to use a large language model for chat. We will demonstrate the features and flexibility of the **llmboost** command in shielding the user from the details of running different models and inference engines on different hardware platform choices. Later tutorials will showcase more powerful and flexible ways to use LLMBoost through LLMBoost's Python programming API and its server deployment options.

### Step 0: Before you start

Start the LLMBoost container.

```bash
# Set environment variables
export MODEL_PATH=<absolute_path_to_model>
export LLMBOOST_IMAGE=<llmboost-docker-image-name>:<tag>
export HUGGING_FACE_HUB_TOKEN=<your_huggingface_token>
```
> ðŸ’¡ These variables are used when launching the Docker container to ensure correct model loading and authentication.

<Tabs>
  <TabItem value="nvidia" label="On NVIDIA" default>

```bash
docker run -it --rm \
  --network host \
  --gpus all \
  --group-add video \
  --ipc host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  -v $MODEL_PATH:/workspace/models \
  -w /workspace \
  -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \
  $LLMBOOST_IMAGE \
  bash
```
</TabItem>

<TabItem value="amd" label="On AMD">
```bash
docker run -it --rm \
  --network host \
  --group-add video \
  --ipc host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --device=/dev/dri:/dev/dri \
  --device=/dev/kfd:/dev/kfd \
  -v $MODEL_PATH:/workspace/models \
  -w /workspace \
  -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \
  $LLMBOOST_IMAGE \
  bash
```
</TabItem> 
</Tabs>

An LLMBoost container should start in an interactive command shell session. The commands in the rest of this tutorial should be entered into the command shell prompt within the container.


### Step 1. Running a simple chatbot (`llmboost chat`)

From the LLMBoost container's command shell prompt, you can invoke the **llmboost** command to perform a number of inference tasks. For example, type the following command to start an interactive chat session using the specified large language model.

```bash
llmboost chat --model_name meta-llama/Llama-3.2-1B-Instruct
```

It can take a few minutes to load a large language model. When presented with a chat prompt, type a few questions to interact with the language model. For example, try asking:
1) What is an LLM?
2) Summarize the previous answer in 1 sentence.
3) Translate the summary into French.

```bash
Initializing LLMBoost...
Preparing model with 8192 context length...
INFO 05-30 23:41:19 [__init__.py:239] Automatically detected platform rocm.
Deploying LLMBoost (this may take a few minutes) .......................................\
[INFO] Model: meta-llama/Llama-3.2-1B-Instruct
[INFO] You can set the system message by typing 'system: <message>'
[INFO] Type 'exit' to quit

[INFO] Welcome to Mango LLMBoost chatbot!
[INFO] What do you want to discuss?
>>> What is an LLM?                                                    
LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model
...
```

When finished, type `exit` or press `ctrl+D` to end the chat session.

### Step 2. Try out different language models
You can try relaunching new chat sessions with different models through the **--model_name** argument.
LLMBoost hides the details of running different models optimally on your hardware setup.
For example, try this smaller model:

```bash
llmboost chat --model_name meta-llama/Llama-3.2-1B-Instruct
```

or try this larger model:

```bash
llmboost chat --model_name meta-llama/Llama-3.1-70B-Instruct
```

Can you see the difference in the models' responsiveness? Larger models will be more expensive to run. Nevertheless, the LLMBoost runtime will automatically configure the software and hardware stack for optimal execution of your model and GPU choice. When you first begin, you can omit most optional arguments and simply allow the LLMBoost runtime to manage the execution configurations for you.

In the next tutorial, we will demonstrate more powerful and flexible ways to use LLMBoost through LLMBoost's Python programming API.

